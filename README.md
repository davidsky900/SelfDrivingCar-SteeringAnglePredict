[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)
# Udacity Self-Driving Car Engineer Nanodegree
# Project: Predict Steering Angle by Behavioral Cloning
---

**Project Guidlines**

The steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report

---
### Required Files

#### Submission if required files
Submission includes all required files and can be used to run the simulator in autonomous mode, specifically, my project includes the following files:
* model.ipynb containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* run1.mp4 as the video output recorded by the front camera of the car in the simulator to test the trained model
* README.md summarizing the results

### Quality of Code 
#### Functionality of code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py modelV2.h5
```

#### Readability of code

The model.ipynb file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

### Model Architecture and Training Strategy

#### Description of model architecture

The structure of the model is summarized in Table 1, and it is included in the model.ipynb. 

As shown in the Table 1, the model includes 9 hidden layers of convolutional neural networks, in conjuction with RELU layers to introduce nonlinearity.  The input data is normalized in the model using a Keras lambda layer, and is cropped to reduce the disturbance of the scene other than the road. 

#### Reduce overfitting in the model

A couple of measures were taken to prevent overfitting in the model, including
* Augmented data were generated by using pictures taken by left, center and right cameras. The steering angles associated with left and right cameras were obtained by add/subtract the steering angle from the middle by a constant of 0.2. All the images were flipped about the center to get a balanced distribution of left / right turns.
* A train_test_split function was used to split the augmented data set into training and validation data set, with the validation set to be 20% of size of augmented data set.

#### Model parameter tuning

The model used an adam optimizer, so the learning rate was not tuned manually. The epoch of training is set to be 5 to avoid overfitting.

#### Training data selection

Training data was generated by the simulator with the driving of the vehicle on the center of the road. The data set is augmented by including the pictures taken by left and right cameras. More details regarding augmenting data, data pre-processing, traing and validation data spliting are included in the next section. 
Below is a summary of the dimention of the data and necessary parameters
* Size of augmented data = 48216
* Size of training data = 38568
* Size of validation data = 9648
* Number of epoch = 5
* Learning rate, adjusted by adam optimizer

### Architecture and Training Documentation

#### Doc - Solution Design Approach

The overall strategy for deriving a model architecture was to adopt existing models with approporiate amount of modification. The structure of the model is from a model published by [NIVIDIA](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/). This model is appropriate because it was demonstrated that it successfully predicted command steering angle with a real autonomous driving car.

In order to gauge how well the model was working, I split the image and steering angle data into a training and validation set. The loss of training and validation is shown in the figure below.

![alt text](https://github.com/davidsky900/SelfDrivingCar-SteeringAnglePredict/blob/master/examples/MESLoss.png)

The final step was to run the simulator to see how well the car was driving around track one. There were a few spots where the vehicle fell off the track, especially when the vehicle was trying to cross the bridge, to improve the driving behavior in these cases, I added data recorded from left and right camera so that the vehicle can learn to correct its position once it deviates from the center.

At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road. A demonstration can be found in the video named as run1.mp4

#### Doc - Final Model Architecture

The final model architecture was shown in Table 1. 

Table 1. Architecture of Final Model

| Layer         		|     Description	        					| 
|:---------------------:|:---------------------------------------------:| 
| Input         		| 160x320x3 RGB image   							| 
| Lambda & Crop         		| 110x30x3 matrices, values range from -0.5 to 0.5   							| 
| Convolution2D    	| CNN 24x5x5 + max pooling 2x2 + RELU	|
| Convolution2D    	| CNN 36x5x5 + max pooling 2x2 + RELU	|
| Convolution2D    	| CNN 48x5x5 + max pooling 2x2 + RELU	|
| Convolution2D    	| CNN 64x5x5 + RELU	|
| Convolution2D    	| CNN 64x5x5 + RELU	|
| Fully connected		| outputs 100x1        									|
| Fully connected		| outputs 50x1        									|
| Fully connected		| outputs 10x1        									|
| Steering Command		| outputs 1x1        									|

#### Doc - Creation of the Training Set & Training Process

To capture good driving behavior, I first recorded two laps on track one using center lane driving. Here is an example image of center lane driving:

![alt text](https://github.com/davidsky900/SelfDrivingCar-SteeringAnglePredict/blob/master/examples/centerEx.jpg)

I then used the images recorded from left and right camera, and the steering angles associated were obtained by add/subtract the steering angle from the middle by a constant of 0.2 deg. With this augmented data set, the vehicle would learn to how to correct its position by proper amount of steering angle. These images show what a recovery looks like steering from left or right :

![alt text](https://github.com/davidsky900/SelfDrivingCar-SteeringAnglePredict/blob/master/examples/leftEx.jpg)
![alt text](https://github.com/davidsky900/SelfDrivingCar-SteeringAnglePredict/blob/master/examples/rightEx.jpg)

To augment the data sat, I also flipped images and angles to balance the distribution of the steering directions (left and right). For example, here is an image that has then been flipped:

![alt text](https://github.com/davidsky900/SelfDrivingCar-SteeringAnglePredict/blob/master/examples/Flips.jpeg)

I then preprocessed this data by changing the color map from BGR to RGB, normalizing the data to value of -0,5 to 0.5 (in the Lambda layer of model), reducing the disturbance introduced by the scenes that are not road by cropping the images by 50x20 pixels. Fianlly, I randomly shuffled the data set and put 20 % of the data into a validation set. The number of epoch of training is set as 5 to provent overfitting and the learning rate is adjuted by the adam optimizer.


### Simulation
In the submitted video, the vehicle did not pop up onto ledges or roll over any sufaces that would otherwise be considered unsafe. 
